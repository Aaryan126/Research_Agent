Literature Review: Memory and Context Management in AI Agent Systems
1. Introduction
Memory and context management represent critical architectural challenges in modern AI agent systems, particularly those powered by large language models (LLMs). As agents evolve from simple reactive systems to sophisticated autonomous entities capable of multi-turn interactions, long-horizon planning, and cross-session continuity, their ability to effectively store, retrieve, and utilize contextual information becomes paramount. This literature review synthesizes findings from a corpus of 122 academic papers on agentic AI, examining how contemporary systems address memory architectures, context management strategies, retrieval mechanisms, and the security challenges inherent in persistent memory systems.

2. Research Landscape
The corpus comprises 122 papers published primarily between 2023-2026, with the majority (101 papers) published in 2025, reflecting the explosive recent growth in agentic AI research. Key foundational work includes the highly-cited "Cognitive Architectures for Language Agents" (Yao et al., 2023, 296 citations) which established the CoALA framework, and "Zep: A Temporal Knowledge Graph Architecture for Agent Memory" (Chalef et al., 2025, 78 citations), which introduced novel graph-based memory approaches. The research landscape shows strong publication momentum, with memory and context management emerging as a central concern across agent architectures, multi-agent systems, and enterprise applications.

3. Key Findings
3.1 Memory Architecture Taxonomies
[SUPPORTED] Contemporary AI agent systems implement hierarchical memory architectures inspired by cognitive science, typically distinguishing between multiple memory types:

The CoALA (Cognitive Architectures for Language Agents) framework (Yao et al., 2023) provides the most comprehensive taxonomy, organizing memory into:

Working Memory: Maintains active, readily available information as symbolic variables for the current decision cycle, persisting across LLM calls as a data structure rather than relying solely on the LLM's context window
Episodic Memory: Stores experiences from earlier decision cycles, including training input-output pairs, history event flows, game trajectories, or other representations of the agent's experiences
Semantic Memory: Stores an agent's knowledge about the world and itself, including facts, reflections on past experiences, and distilled insights
Procedural Memory: Stores the production system itself—the set of rules and skills that can be applied to determine agent behavior
[SUPPORTED] This multi-layered approach is widely adopted across frameworks. A comparative analysis by the "Agentic AI Frameworks" survey (2025) found that leading frameworks implement varying combinations: LangGraph provides stateful graph nodes for short-term memory; CrewAI supports agent-level memory with entity/contextual memory for both short-term and long-term storage plus episodic memory; AutoGen maintains shared memory context across structured dialogues; and Semantic Kernel offers extensible memory modules integrated with planners supporting short-term, long-term, semantic, and procedural memory.

3.2 Context Window Limitations and Management Strategies
[SUPPORTED] Context window constraints represent a fundamental bottleneck in agent memory systems. As noted in "AI Agents: Evolution, Architecture, and Real-World Applications" (Krishnan, 2025), working memory in language model-based agents is often implemented through context windows that include recent interaction history, but "the effectiveness of this approach is limited by maximum context length constraints."

[SUPPORTED] To address these limitations, systems employ multiple strategies:

The AOI (AI-Oriented Operations) framework (2025) demonstrates sophisticated context compression achieving 72.4% compression while preserving 92.8% critical information through:

LLM-based sliding window compression with 50% overlap to ensure context continuity
Three-layer memory architecture: raw context storage (24-hour retention), task queue management, and compressed context cache (7-day retention)
Intelligent prioritization of fault signatures, error codes, and performance anomalies
[SUPPORTED] The Model Context Protocol (MCP) research (Krishnan, 2025) advocates for externalizing context management rather than embedding all context within model inputs, stating: "Many previous approaches attempted to embed all relevant context within the model's input, leading to context window limitations and inefficient use of model capacity. MCP externalizes context management, storing information outside the model and retrieving it selectively based on relevance."

3.3 Retrieval Mechanisms and RAG Integration
[SUPPORTED] Retrieval-Augmented Generation (RAG) has emerged as the dominant paradigm for augmenting agent memory with external knowledge. Systems typically employ embedding-based retrieval using vector databases, with semantic search enabling agents to retrieve relevant information based on query similarity.

[SUPPORTED] The Zep system (Chalef et al., 2025) represents a significant advancement, introducing temporal knowledge graphs for agent memory. Zep outperforms the previous state-of-the-art system MemGPT on the Deep Memory Retrieval (DMR) benchmark (94.8% vs 93.4%) while reducing response times by approximately 90%. The system implements:

Dynamic, temporally-aware knowledge graphs (Graphiti engine) that synthesize both unstructured conversational data and structured business data
Bi-temporal modeling: one timeline for chronological event ordering, another for transactional data ingestion order
Multiple reranking strategies: graph-based episode-mentions reranker, node distance reranker, and cross-encoder LLMs for relevance scoring
[SUPPORTED] Framework implementations vary: LlamaIndex provides embedding-based context retrieval from large-scale indexed data; systems use BGE-m3 models for embedding and reranking tasks; and hybrid retrieval strategies combine Maximal Marginal Relevance (MMR) with specialized reranking to ensure both diversity and relevance.

3.4 Memory Persistence and Lifecycle Management
[SUPPORTED] Advanced systems implement sophisticated memory lifecycle policies to balance retention with computational efficiency. Research identifies several key strategies:

Utility-Based Retention: Preservation of context based on estimated future utility, with lower-utility information becoming candidates for removal or archival (Model Context Protocol research, 2025).

Importance-Weighted Decay: Gradual reduction in accessibility based on importance and time, implemented through exponential decay functions with importance-adjusted rates, staged demotion across storage tiers, and preservation thresholds for critical information (Model Context Protocol research, 2025).

Hierarchical Memory with Forgetting Mechanisms: The Continuity Dimension framework (2025) establishes scoring criteria where the highest-tier systems (score 3) implement "hierarchical memory with forgetting mechanisms and knowledge consolidation," compared to lower tiers with no memory retention or only session-based memory.

[SUPPORTED] The AOI system demonstrates practical lifecycle management with its three-layer architecture: Layer 1 stores raw context with 24-hour retention, Layer 2 maintains task queues with lifecycle management, and Layer 3 houses compressed context with 7-day retention for strategic decision-making.

3.5 Knowledge Graph Approaches to Memory
[SUPPORTED] Graph-based memory representations are emerging as a powerful alternative to traditional vector-based approaches. The Zep/Graphiti system (Chalef et al., 2025) pioneered temporal knowledge graphs where memory is represented as G = (N, E, φ), with nodes N representing entities, edges E representing relationships, and φ representing the incidence function. The system maintains:

Episodic edges connecting episodes to extracted entity nodes
Bidirectional indices tracking relationships between edges and source episodes
Temporal validity periods for facts and relationships, enabling non-lossy updates
[SUPPORTED] The "Agent KB" system (2025) introduces a universal memory infrastructure enabling cross-framework experience sharing through structured knowledge bases, demonstrating that graph-based approaches can facilitate collective agent intelligence across heterogeneous systems.

3.6 Security Vulnerabilities in Memory Systems
[CONTESTED] Memory systems introduce significant security risks, though the severity and mitigation strategies remain actively debated in the literature.

Memory Poisoning: Multiple papers document memory poisoning attacks where adversaries insert malicious content into agent memory. The "Survey of LLM-Driven AI Agent Communication" (2025) describes how attackers can "corrupt the semantic integrity of the agent's memory store by implanting example pairs that embed adversarial triggers and payloads." These attacks exploit similarity-based retrieval mechanisms, with triggers crafted to "maximize retrieval likelihood under adversarial prompts while maintaining normal performance under benign inputs."

Memory Extraction: The same survey notes that "memory extraction can occur even without explicit queries for private content, instead relying on semantic proximity in the vector space to surface related sensitive traces," particularly dangerous in black-box settings.

Persistent Memory Drift: The "Cognitive Degradation in Agentic AI" paper (2025) documents through empirical testing that "Mixtral and Claude stored hallucinated content in memory and reused it across sessions, confirming cross-session memory poisoning vulnerabilities."

[CONTESTED] However, defensive mechanisms show varying effectiveness. The TRiSM framework (Raza et al., 2025) proposes embedding-level screening, consensus aggregation, and architectural isolation, while the QSAF (Qorvex Security AI Framework) implements memory integrity validation and quarantine mechanisms. The "Survey of LLM-Driven AI Agent Communication" suggests that "unified provenance tracking across both memory and retrieval pipelines enables smarter decisions about retention, ranking, or discounting of contentious content," but acknowledges that "adaptive schemas to detect context-sensitive triggers or stealthy distributional shifts" remain necessary.

4. Contradictions and Debates
4.1 Structured vs. Unstructured Memory Representations
[CONTESTED] A fundamental tension exists between structured (graph-based) and unstructured (vector-based) memory approaches:

Pro-Graph Position: The Zep research (Chalef et al., 2025) argues that "current RAG approaches are unsuitable" for dynamic agent memory because "entire conversation histories, business datasets, and other domain-specific content cannot fit effectively inside LLM context windows." Their temporal knowledge graph approach demonstrates superior performance (94.8% vs 93.4%) and 90% latency reduction compared to vector-based MemGPT.

Pro-Vector Position: The Model Context Protocol research (2025) notes that "earlier approaches often treated context as unstructured text, limiting the model's ability to distinguish between different types of contextual information," but advocates for structured primitives (prompts, resources, tools) with defined semantics rather than full knowledge graphs. Many production systems continue using vector databases with embedding-based retrieval due to implementation simplicity.

Unresolved: No consensus exists on optimal memory representation. The Zep paper acknowledges that "current research on LLM-generated knowledge graphs has primarily operated without formal ontologies," and suggests "domain-specific ontologies present significant potential" but require further exploration.

4.2 Centralized vs. Distributed Memory Architectures
[CONTESTED] Multi-agent systems face architectural choices about memory sharing:

The "Agentic AI Frameworks" survey (2025) documents that frameworks implement varying approaches: AutoGen uses "shared memory context maintained across structured dialogues," while Google ADK provides "shared memory across agent instances and system modules." However, the "Revisiting Gossip Protocols" paper (2025) argues that "structured communication protocols excel at reliability and task delegation, but they fall short in enabling emergent, swarm-like intelligence," advocating for decentralized gossip-based memory dissemination.

Unresolved: The trade-offs between centralized memory (easier consistency, potential bottleneck) and distributed memory (scalability, coordination complexity) remain under-explored, with limited empirical comparisons.

4.3 Memory Security: Severity and Mitigation Effectiveness
[CONTESTED] While papers universally acknowledge memory security risks, assessments of severity and mitigation effectiveness diverge:

The "Threat Model for GenAI Agents" paper (2025) emphasizes that memory threats have "delayed effects" where "initial compromises might only influence future actions days, weeks, or months later, making it extremely difficult to trace incidents back to the root cause." It classifies memory poisoning as high likelihood, critical impact.

However, the "TRiSM for Agentic AI" paper (Raza et al., 2025) presents multiple defensive mechanisms (embedding-level screening, consensus filtering, architectural isolation) suggesting tractable solutions. The effectiveness gap remains: the "Cognitive Degradation" paper's empirical testing found that "Mixtral and Claude stored hallucinated content in memory and reused it across sessions," indicating current defenses are insufficient.

5. Research Gaps
5.1 Standardized Memory Benchmarks
[INSUFFICIENT] The Zep paper (Chalef et al., 2025) explicitly notes: "Notably, no existing benchmarks adequately assess Zep's capability to process and synthesize conversation history with structured business data." Current benchmarks (DMR, LongMemEval) focus on conversational memory but lack evaluation of:

Cross-modal memory integration (text, structured data, multimodal inputs)
Memory performance under adversarial conditions
Long-term memory degradation and drift
Memory scalability in production systems with millions of interactions
5.2 Forgetting Mechanisms and Memory Optimization
[INSUFFICIENT] While the Model Context Protocol research (2025) outlines theoretical forgetting strategies (utility-based retention, importance-weighted decay, summarization), empirical validation is sparse. Only one paper in the corpus mentions "forgetting mechanisms" in scoring criteria, with no detailed implementation studies or comparative evaluations of different forgetting policies.

5.3 Cross-Framework Memory Interoperability
[INSUFFICIENT] The Agent KB paper (2025) identifies that "AI agent frameworks operate in isolation, forcing agents to rediscover solutions and repeat mistakes across different systems," but provides only initial solutions. Research gaps include:

Standardized memory serialization formats across frameworks
Memory migration and translation between different architectural paradigms
Federated memory systems enabling privacy-preserving memory sharing
Memory versioning and compatibility across agent system updates
5.4 Temporal Reasoning in Memory Systems
[INSUFFICIENT] While Zep introduces bi-temporal modeling, broader temporal reasoning capabilities remain under-explored:

Handling contradictory information across time (belief revision)
Temporal query languages for agent memory
Causal reasoning over temporal memory graphs
Time-aware memory consolidation and abstraction
5.5 Memory-Augmented Learning and Adaptation
[INSUFFICIENT] The relationship between memory systems and agent learning receives limited attention. The "Agentic AI Frameworks" survey notes that "some AMAS include learning components that adapt behavior over time," but detailed studies of:

How memory systems support continual learning without catastrophic forgetting
Integration of episodic memory with reinforcement learning
Memory-guided exploration strategies
Transfer learning through shared memory infrastructures
are largely absent from the current literature.

5.6 Production System Scalability
[INSUFFICIENT] The Zep paper explicitly states: "Current literature on LLM memory and RAG systems insufficiently addresses production system scalability in terms of cost and latency." Critical gaps include:

Memory system performance at enterprise scale (millions of users, billions of interactions)
Cost-performance trade-offs for different memory architectures
Memory system reliability, fault tolerance, and disaster recovery
Real-time memory updates in high-throughput scenarios
6. Suggested Future Research Directions
Based on identified gaps and contested areas, future research should prioritize:

Comprehensive Memory Benchmarking Suite: Develop standardized benchmarks evaluating memory systems across dimensions of accuracy, latency, scalability, security robustness, cross-modal integration, and long-term stability.

Adaptive Forgetting Mechanisms: Conduct empirical studies comparing forgetting strategies (utility-based, importance-weighted, summarization-based) across diverse agent tasks, establishing best practices for memory lifecycle management.

Secure Memory Architectures: Design and validate memory systems with built-in security properties, including provenance tracking, anomaly detection, consensus-based validation, and cryptographic integrity guarantees.

Hybrid Memory Representations: Explore architectures combining strengths of vector-based and graph-based approaches, potentially using graphs for structured knowledge and vectors for semantic similarity, with intelligent routing between representations.

Temporal Memory Reasoning: Develop formal frameworks for temporal reasoning over agent memory, including belief revision under temporal constraints, causal inference from memory traces, and time-aware memory consolidation.

Cross-Framework Memory Standards: Establish interoperability standards enabling memory sharing and migration across heterogeneous agent frameworks, potentially through standardized memory APIs and serialization formats.

Production-Scale Memory Systems: Conduct large-scale empirical studies of memory system performance in production environments, establishing engineering best practices for reliability, cost-efficiency, and operational excellence.

References
Yao, S., Sumers, T., Narasimhan, K., & Griffiths, T. L. (2023). Cognitive Architectures for Language Agents. Transactions on Machine Learning Research, paper_id: e4bb1b1f97711a7634bf4bff72c56891be2222e6.

Chalef, D., Ryan, J., Paliychuk, P., Rasmussen, P., & Beauvais, T. (2025). Zep: A Temporal Knowledge Graph Architecture for Agent Memory. paper_id: 12407be490a4e4633da7f25a93af000be573a288.

Krishnan, N. (2025). AI Agents: Evolution, Architecture, and Real-World Applications. paper_id: 38dade87db5fa0d6771121940e079bef774c9400.

Krishnan, N. (2025). Advancing Multi-Agent Systems Through Model Context Protocol: Architecture, Implementation, and Applications. paper_id: 66098cff05d84454084aa442640c98c076f375f2.

(2025). Agentic AI Frameworks: Architectures, Protocols, and Design Challenges. paper_id: 4740a5403d308fea74f7d4f9667d07acacc19190.

(2025). AOI: Context-Aware Multi-Agent Operations via Dynamic Scheduling and Hierarchical Memory Compression. paper_id: a9398f66e35132fbe3e4d6710359e7d183af5ec6.

(2025). Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving. Authors: Wu, C., Wang, C., Wu, F., Zhang, G., et al. paper_id: d4b9917737a520c471a7dd0e0e80520b7508562a.

(2025). Episodic Memory in Agentic Frameworks: Suggesting Next Tasks. paper_id: feb3c862a3dca995050feca08abbf07975f88d06.

Raza, S., Sapkota, R., Emmanouilidis, C., & Karkee, M. (2025). TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems. paper_id: 753736d18fa9bf3ed730836b30b89bf5653cd8dd.

(2025). A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures. Authors: Lin, C., Chen, C., Kong, D., et al. paper_id: efebe807c65b8f640b6f34c2910e81b8b9f7f7c5.

(2025). Cognitive Degradation in Agentic AI. paper_id: c3da993174d74465fbb632f2d3de5c31d8b58a26.

(2025). A Comprehensive Threat Model Tailored Specifically for GenAI Agents. paper_id: 9202c1bfbf620f9f760d96003ff5f13c20163537.

(2025). Revisiting Gossip Protocols: A Vision for Emergent Coordination in Agentic Multi-Agent Systems. paper_id: ab8713d70faf9465ef12985f306d3e6692c499bc.

(2025). The Evolution of Agentic AI in Cybersecurity: From Single LLM Reasoners to Multi-Agent Systems and Autonomous Pipelines. paper_id: 6417c2e047247e4e844c8ab390fb3812fb4f80e1.

(2024). AIOS: LLM Agent Operating System. Authors: Mei, K., Ye, R., Xu, S., et al. paper_id: f89e85059a55b647c93822aefa7e985376e0ef20.

(2024). The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey. Authors: Chao, A., Sawtell, M., Besen, S., & Masterman, T. paper_id: 25ae2fce719c6f6f0b09de1e0f917a7b719e5e99.

(2025). Continuity Dimension Scoring Framework. paper_id: da3f3658294e91e45a47c521ba2da2e22045bd70.

(2025). ASTRIDE: AI Agent-Specific Threat Framework. paper_id: 37c56b578604d3dea4d86ae9bf1a76cdbff5129e.

(2025). AI Agent Security Analysis. paper_id: d867069f8d26441226f3377eaa5cd4f2ba109216.

(2024). Memory Processing in AI Agents. paper_id: 4c21f660843d4ce4041ffb4af073a2d7b1e0b61d.