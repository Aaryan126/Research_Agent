Literature Review: Multi-Agent Systems for Hallucination Handling in Large Language Models
1. Introduction — Topic Overview and Scope
This literature review examines how multi-agent systems address hallucination in large language models (LLMs), drawing from a corpus of approximately 200 academic papers on Agentic AI. Hallucinations—plausible but factually incorrect outputs—represent a critical reliability challenge for LLM deployment. The review synthesizes research on architectural patterns, detection mechanisms, and mitigation strategies employed by multi-agent frameworks to reduce hallucination rates and improve factual accuracy.

2. Research Landscape — Corpus Statistics and Publication Trends
The corpus contains 122 papers total, with the vast majority published in 2025 (101 papers) and 2024 (11 papers), reflecting the recent surge in agentic AI research.

Multi-agent systems research shows strong representation with 119 papers mentioning multi-agent systems in their abstracts, distributed as:

2020: 1 paper
2023: 2 papers
2024: 11 papers
2025: 98 papers
2026: 7 papers
Hallucination-focused research appears in 79 papers:

2023: 2 papers
2024: 7 papers
2025: 67 papers
2026: 3 papers
This publication pattern indicates that multi-agent approaches to hallucination mitigation emerged as a major research focus primarily in 2024-2025, representing a very recent but rapidly expanding area of investigation.

3. Key Findings — Organized by Strategy Cluster
3.1 Cross-Agent Verification and Consensus Mechanisms
Multi-agent consensus reduces hallucinations through collaborative validation. The LLM-Consensus system (Lakara et al. 2025) demonstrates that combining multi-agent reasoning with external retrieval improves explainability and reduces hallucinations. The MAD-Sherlock framework (Lakara et al. 2025) employs a debate-driven system where agents collaborate to reduce hallucinations and strengthen fact-verification, achieving state-of-the-art accuracy without domain-specific fine-tuning.

A multi-agent framework for misinformation detection (Gautam 2025) showed that cross-validation across specialized agents—including Retrieval, Detective, and Analyst agents—improves detection accuracy through structured workflows. The system employs five specialized agents working in pipeline: an Indexer for maintaining trusted repositories, a Classifier for labeling misinformation types, an Extractor for evidence-based retrieval, a Corrector for generating fact-based corrections, and a Verification agent for validating outputs.

Quantitative evidence: A prompt injection mitigation study using multi-agent pipelines demonstrated a 28× reduction factor in Total Hallucination Score (THS), corresponding to approximately 96% reduction in hallucinations. The system showed progressive improvement across three agent stages: THS1 (Mean = -0.0049), THS2 (Mean = -0.0456), and THS3 (Mean = -0.1396), with the final stage achieving the strongest mitigation effect.

3.2 Specialized Agent Roles and Division of Labor
Role specialization enables targeted hallucination detection and correction. The FactAgent framework (Li, Zhang, and Malthouse 2024) modularizes fact-checking into specialized components for evidence retrieval, temporal verification, and source cross-referencing, enhancing interpretability in veracity assessment.

A multi-agent NLP framework for prompt injection (extending from hallucination mitigation work) employs distinct roles:

Front-End Generator: Produces initial responses
Guard/Sanitizer: Detects and sanitizes problematic outputs
Policy Enforcer: Ensures compliance with factual grounding requirements
KPI Evaluation Agent: Monitors system performance
This division of labor allows each agent to specialize in one aspect of the hallucination mitigation pipeline rather than attempting to handle all tasks within a single architecture, improving transparency, explainability, and robustness.

3.3 Sentinel Agents for Distributed Monitoring
Sentinel Agents provide continuous, distributed hallucination detection. Research by Gosmar and Dahl (2025) introduces Sentinel Agents as autonomous monitors functioning as a distributed security layer that integrates semantic analysis via LLMs, behavioral analytics, retrieval-augmented verification, and cross-agent anomaly detection. These agents oversee inter-agent communications, identify potential threats including hallucinations, and maintain comprehensive audit records.

In a simulation study with 162 synthetic attacks across three families (prompt injection, hallucination, and data exfiltration), Sentinel Agents achieved 100% detection rate, though the authors caution that this was a limited testbed without false positive measurement. The dual-layered approach combining continuous monitoring of Sentinel Agents with governance functions of Coordinator Agents supports dynamic defense mechanisms against hallucinations generated by LLMs.

3.4 Retrieval-Augmented Generation (RAG) Integration
External knowledge grounding through RAG reduces parametric hallucinations. Multiple papers identify RAG as a core strategy for multi-agent hallucination mitigation. RAG systems augment LLM generation by sourcing passages from external knowledge bases rather than relying solely on parametric knowledge stored in pretrained models.

The MOSAIC system (Multi-framework Structured Agentic AI for Clinical Communication) employs a Verification Agent that provides consistency checks and feedback, working alongside Annotation Agents that apply codebook-guided RAG with dynamic few-shot prompting. This architecture demonstrates how retrieval-based grounding acts as an auxiliary supervision signal, enabling improvement without manual annotation or gradient updates.

Research on Image-RAG for multi-agent systems shows that retrieval-based grounding compensates for hallucination or semantic ambiguity common in zero-shot models by structuring retrieved information around interpretable class definitions and visual prototypes. The reevaluation loop powered by RAG enables agents to reflect on prior decisions in light of retrieved evidence, with optimal outcomes achieved through meta-reasoning arbitration.

3.5 Cognitive Degradation Detection
Internal monitoring prevents systemic hallucination drift. The QSAF framework (Cognitive Degradation mitigation) introduces a novel vulnerability class distinct from external adversarial threats. Cognitive Degradation failures originate internally from memory starvation, planner recursion, context flooding, and output suppression, leading to silent agent drift, logic collapse, and persistent hallucinations over time.

The framework includes seven runtime controls that monitor agent subsystems in real time and trigger proactive mitigation through fallback routing, starvation detection, and memory integrity enforcement. By mapping agentic architectures to human cognitive analogs, the system enables early detection of fatigue, starvation, and role collapse that contribute to hallucination.

4. Contradictions and Debates
4.1 Detection Difficulty: Hallucination vs. Other Vulnerabilities
CONTESTED: Research reveals that hallucinations are easier to detect than other AI safety issues, but opinions differ on mitigation complexity.

One study found that hallucinations "often manifest as clearly identifiable factual inaccuracies, making them easier to detect and remediate using multi-agent pipelines that cross-verify information and enforce factual grounding." The 96% reduction in hallucination scores supports this view.

However, the same research notes that while detection may be easier, "different detection and remediation strategies are required for each" type of vulnerability. For hallucination mitigation, "rigorous fact-checking and context validation are crucial," suggesting that even if detection is straightforward, comprehensive mitigation remains challenging.

4.2 Centralized vs. Decentralized Coordination
CONTESTED: The optimal orchestration pattern for multi-agent hallucination mitigation remains debated.

Some frameworks advocate centralized coordination where a master agent manages communication and policy enforcement, ensuring adherence to guidelines. The Coordinator Agent model exemplifies this approach, supervising policy implementation and ingesting alerts from Sentinel Agents.

Others argue for decentralized approaches where agents operate more autonomously. The research notes that centralized models ensure policy adherence "at the cost of potential latency," while decentralized models may offer faster response times but complicate coordination. The corpus shows active exploration of both patterns without clear consensus on superiority.

4.3 RAG Limitations and Vulnerabilities
CONTESTED: While RAG is widely adopted for hallucination mitigation, researchers identify significant security and reliability concerns.

RAG systems are praised for grounding generation in external knowledge, with multiple papers demonstrating effectiveness in reducing hallucinations. However, critical analysis reveals that "the information retrieval module—serving as the agent's external memory—becomes an adversarial surface where unverified or manipulable corpora may be exploited."

Specific vulnerabilities include:

Knowledge corruption via data poisoning: Adversarial texts can be injected into retrieval corpora
Static limitations: Traditional RAG approaches "remain largely passive and centrally constrained"
Equal weighting problems: Most RAG implementations "treat the retrieval step as static and apply equal weight to all retrieved content"
Advanced systems address these concerns through dynamic trust scoring and iterative reevaluation loops, but the fundamental tension between retrieval benefits and retrieval vulnerabilities remains unresolved.

5. Research Gaps — Areas with Insufficient Evidence
5.1 Long-term Hallucination Drift
INSUFFICIENT: Only one paper (QSAF framework) addresses persistent hallucinations that emerge over extended agent operation. The concept of "Cognitive Degradation" leading to "silent agent drift" and "persistent hallucinations over time" is introduced but lacks extensive empirical validation across diverse agent architectures and deployment scenarios.

5.2 Cross-Domain Generalization
INSUFFICIENT: Most studies focus on specific domains (healthcare, misinformation detection, customer service). Evidence is lacking on whether multi-agent hallucination mitigation strategies generalize across domains or require domain-specific customization. The corpus contains domain-specific implementations but minimal comparative analysis.

5.3 Computational Cost-Benefit Analysis
INSUFFICIENT: While one study reports that multi-agent evolutionary frameworks cost "$2.07 to $11.30" per experiment, comprehensive analysis of the computational overhead introduced by multi-agent hallucination mitigation is sparse. The misinformation detection framework acknowledges "managing inter-agent coordination, mitigating latency introduced by multi-stage processing" as challenges, but quantitative cost-benefit comparisons are absent.

5.4 Human-in-the-Loop Integration
INSUFFICIENT: Several papers mention human oversight (e.g., "Increasing human oversight to validate and refine the automated detection and sanitization processes"), but systematic research on optimal human-agent collaboration patterns for hallucination mitigation is limited. The balance between automation and human verification remains underexplored.

5.5 False Positive Rates
INSUFFICIENT: The Sentinel Agent evaluation explicitly notes that "false positives were not measured" despite achieving 100% detection rates. This represents a critical gap—aggressive hallucination detection may flag correct outputs as hallucinations, but the corpus provides minimal data on false positive rates across different multi-agent architectures.

6. Suggested Future Research Directions
Standardized Benchmarking: Develop unified metrics and datasets for comparing multi-agent hallucination mitigation approaches across architectures, enabling reproducible performance comparisons.

Hybrid Orchestration Models: Investigate adaptive coordination patterns that dynamically switch between centralized and decentralized control based on task requirements and latency constraints.

Adversarial RAG Robustness: Develop cryptographic verification and provenance tracking for retrieval corpora to prevent knowledge poisoning attacks that could induce hallucinations.

Cross-Domain Transfer Learning: Systematically evaluate which hallucination mitigation strategies transfer across domains and which require domain-specific adaptation.

Cognitive Degradation Monitoring: Expand research on internal failure modes that cause hallucination drift over extended operation, developing predictive models for agent fatigue and memory corruption.

Cost-Optimized Architectures: Design lightweight multi-agent configurations that balance hallucination reduction with computational efficiency, particularly for resource-constrained deployment environments.

Explainable Verification: Enhance transparency in multi-agent hallucination detection by developing interpretable verification trails that allow users to understand why specific outputs were flagged or validated.

Neuro-Symbolic Integration: Explore hybrid approaches combining neural pattern recognition for hallucination detection with symbolic reasoning for formal verification of factual claims.

Methodological Note: This review synthesized evidence from the top-cited papers and targeted searches across the corpus. All claims are grounded in specific papers cited by title and year. The rapid publication growth in 2024-2025 indicates this remains an actively evolving research area where consensus is still forming on optimal approaches.