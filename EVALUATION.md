# Evaluation Script for Research Agent Reports

## Purpose

`evaluate_reports.py` measures the quality of literature review reports generated by the Academic Research Agent. It cross-references each report against the Elasticsearch indexes (`papers_metadata` and `papers_chunks`) that the agent used during research, answering a core question: **did the agent faithfully represent the source material, or did it fabricate information?**

Each metric targets a specific failure mode of AI-generated academic output:

| Metric | What it detects |
|---|---|
| Citation Accuracy | Hallucinated references — papers that don't exist in the corpus |
| Claim Grounding | Fabricated statistics — numbers that can't be traced to source text |
| Corpus Coverage | Tunnel vision — over-reliance on a small subset of available papers |
| Confidence Distribution | Miscalibration — whether the agent properly classifies evidence strength |
| Report Statistics | Structural completeness — whether the report covers expected sections |

## Metrics Explained

### Citation Accuracy

Parses the References section and verifies each cited paper exists in `papers_metadata`.

- **paper_id lookup** (primary): If the reference includes a paper_id, performs a term query to confirm it exists.
- **Title match** (fallback): If no paper_id is present, performs a text match on the title field.
- **Hallucinated citations**: Any reference that cannot be verified by either method. These represent papers the agent may have fabricated.

### Claim Grounding

Scans the report body for quantitative claims (percentages, multipliers, specific counts, comparisons) and checks whether the surrounding context can be found in the actual paper text stored in `papers_chunks`.

- Extracts ~10 descriptive words around each claim as a search query
- Uses BM25 text matching against chunk text — no embeddings required
- A claim is "grounded" if it matches source text above a relevance threshold

### Corpus Coverage

Measures what fraction of the total paper corpus the agent cited in its review.

- Counts unique paper_ids in the References section
- Compares against total document count in `papers_metadata`
- A single review is expected to cover a subset, not the entire corpus — this metric is most useful for comparing across reviews or tracking whether the agent is exploring broadly

### Confidence Distribution

Counts occurrences of the agent's confidence tags:

- `[SUPPORTED]` — Multiple papers with empirical evidence
- `[CONTESTED]` — Conflicting findings across the literature
- `[INSUFFICIENT]` — Limited evidence or single-source claims

A well-calibrated agent should produce a mix of all three tags, not exclusively `[SUPPORTED]`.

### Report Statistics

Structural metrics for completeness:

- **Word count**: Overall report length
- **Sections**: Number of main section headers
- **References**: Total papers cited
- **Research gaps**: Items identified in the Research Gaps section
- **Contradictions**: Items identified in the Contradictions section

## How to Run

### Prerequisites

- Python 3.8+
- `elasticsearch` and `python-dotenv` packages installed
- `.env` file configured with Elasticsearch connection details (see `.env.example`)

### Single Report

```bash
python evaluate_reports.py --file reports/report_hallucination.txt
```

### All Reports (Batch Mode)

```bash
python evaluate_reports.py --all
```

Batch mode evaluates every `.txt` file in `reports/`, then prints an aggregate summary with averages across all reports.

### Output

Each evaluation prints a formatted summary to the console and saves a JSON file at `reports/{report_name}_eval.json` with the full results.

## How to Interpret Results

### What Good Looks Like

| Metric | Good | Concerning |
|---|---|---|
| Citation accuracy | 90-100% | Below 80% — agent may be inventing references |
| Claim grounding | 70-100% | Below 50% — agent may be fabricating statistics |
| Coverage per review | 8-20% | Below 5% — agent may be over-relying on few papers |
| Confidence mix | All three tags present | Only `[SUPPORTED]` — agent isn't critically assessing evidence |
| Research gaps | 3+ identified | 0 — agent may not be identifying limitations |
| Contradictions | 1+ identified | 0 — agent may not be surfacing scholarly disagreements |

### What Bad Numbers Indicate

- **Low citation accuracy** (< 80%): The agent is hallucinating references — citing papers that don't exist in the indexed corpus. This is the most serious quality failure.
- **Low claim grounding** (< 50%): The agent is generating statistics or quantitative claims that can't be traced back to the actual paper text. These may be fabricated or heavily paraphrased beyond recognition.
- **Very low coverage** (< 5%): The agent is producing a "review" from only a handful of papers despite having access to a larger corpus. This suggests the retrieval pipeline may not be surfacing diverse results.
- **Only `[SUPPORTED]` tags**: The agent isn't applying critical analysis. A legitimate literature review should identify contested findings and evidence gaps, not present everything as settled fact.
- **Zero contradictions or gaps**: A corpus of 100+ papers on an active research topic will contain disagreements. If the agent finds none, it's likely not looking hard enough.

### Claim Grounding Caveats

The grounding check uses keyword matching, not semantic similarity. Some claims may appear "unverified" because:

- The agent paraphrased heavily (correct behavior, but hard to trace)
- The claim synthesizes across multiple papers (legitimate but un-matchable to a single chunk)
- The claim comes from tables or figures that weren't captured during PDF parsing

A grounding rate of 70-80% is healthy. Rates below 50% warrant manual inspection of the unverified claims.

## Sample Output

```
======================================================
  EVALUATION: report_security.txt
======================================================

  CITATION ACCURACY
    Total citations:            15
    Verified by paper_id:       15
    Verified by title match:    0
    Not found:                  0
    Citation accuracy:          100.0%
    Hallucinated citations:     0

  CLAIM GROUNDING
    Quantitative claims found:  18
    Verified in corpus:         14
    Unverified:                 4
    Grounding rate:             77.8%

  CORPUS COVERAGE
    Unique papers cited:        15
    Total papers in corpus:     122
    Coverage per review:        12.3%

  CONFIDENCE DISTRIBUTION
    [SUPPORTED]:                14 (63.6%)
    [CONTESTED]:                5  (22.7%)
    [INSUFFICIENT]:             3  (13.6%)

  REPORT STATISTICS
    Word count:                 5,842
    Sections:                   8
    References:                 15
    Research gaps:              6
    Contradictions:             3

======================================================

======================================================
  AGGREGATE SUMMARY (3 reports)
======================================================
  Avg citation accuracy:       98.5%
  Avg claim grounding:         75.2%
  Avg papers cited per review: 15
  Avg word count:              5,200
  Total unique papers cited:   44 / 122
  Avg [SUPPORTED] per review:  14
  Avg [CONTESTED] per review:  4
  Avg [INSUFFICIENT] per review: 5
======================================================
```

## Connection to the Agent Pipeline

This evaluation validates the agent's multi-step research pipeline:

1. **Retrieval quality** → Corpus Coverage measures whether the agent's search queries are surfacing diverse, relevant papers from Elasticsearch
2. **Comprehension accuracy** → Claim Grounding measures whether the agent accurately extracts and reports quantitative findings from paper text
3. **Citation integrity** → Citation Accuracy measures whether the agent's references point to real papers, not hallucinated sources
4. **Critical analysis** → Confidence Distribution measures whether the agent appropriately classifies evidence strength rather than presenting everything as established fact
5. **Structural completeness** → Report Statistics measure whether the agent produces a well-organized review with gaps, contradictions, and proper references

Together, these metrics provide a quantitative assessment of each stage where the agent could introduce errors — from retrieval through synthesis to final output.
